有些词在文本中尽管词频高，但是并不重要，这个时候就可以用TF-IDF技术，进行特征的权重修正。

#### 1.1 简介
       TF-IDF（Term Frequency–Inverse Document Frequency），即“词频-逆文本频率”，是一种用于资讯检索与文本挖掘的常用加权技术。它由两部分组成，TF和IDF。

		TF-IDF是一种统计方法，用以评估一个字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。

       TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TF-IDF实际上是：TF * IDF。

     （1）词频（Term Frequency，TF）：指的是某一个给定的词语在该文件中出现的频率。
     （2）逆向文件频率（Inverse Document Frequency，IDF）：是一个词语普遍重要性的度量，强调词对文档的区分能力。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如“to”；反过来，如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词如“Machine Learning”。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。公式如下：

		其中，N代表语料库中文本的总数，而N(x)代表语料库中包含词x的文本总数。
		上面的IDF公式已经可以使用了，但是在一些特殊的情况会有一些小问题，比如某一个生僻词在语料库中没有，这样分母为0， IDF没有意义了。所以常用的IDF需要做一些平滑，使语料库中没有出现的词也可以得到一个合适的IDF值。平滑的方法有很多种，最常见的IDF平滑公式之一为：
		某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。
		高权重的TF-IDF  = 某一特定文件内的高词语频率 + 该词语在整个文件集合中的低文件频率
#### 4.2 用sklearn进行TF-IDF预处理
		第一种方法是在用 CountVectorizer 类向量化之后再调用 TfidfTransformer 类进行预处理。第二种方法是直接用 TfidfVectorizer 完成向量化与 TF-IDF 预处理。
		(1) CountVectorizer 结合 TfidfTransformer
```
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
 
corpus = ['This is the first document.',
          'This is the second second document.',
          'And the third one.',
          'Is this the first document?',
          ]
          
# 词袋化
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
print(X)
 
# TF-IDF
transformer = TfidfTransformer()
tfidf = transformer.fit_transform(X)
print(tfidf)
```
		词袋化结果：

```
(0, 1)    1
(0, 2)    1
(0, 6)    1
(0, 3)    1
(0, 8)    1
(1, 5)    2
(1, 1)    1
(1, 6)    1
(1, 3)    1
(1, 8)    1
(2, 4)    1
(2, 7)    1
(2, 0)    1
(2, 6)    1
(3, 1)    1
(3, 2)    1
(3, 6)    1
(3, 3)    1
(3, 8)    1
```

		输出的各个文本各个词的 TF-IDF 值如下：

```
(0, 8)    0.4387767428592343
(0, 3)    0.4387767428592343
(0, 6)    0.35872873824808993
(0, 2)    0.5419765697264572
(0, 1)    0.4387767428592343
(1, 8)    0.27230146752334033
(1, 3)    0.27230146752334033
(1, 6)    0.2226242923251039
(1, 1)    0.27230146752334033
(1, 5)    0.8532257361452784
(2, 6)    0.2884767487500274
(2, 0)    0.5528053199908667
(2, 7)    0.5528053199908667
(2, 4)    0.5528053199908667
(3, 8)    0.4387767428592343
(3, 3)    0.4387767428592343
(3, 6)    0.35872873824808993
(3, 2)    0.5419765697264572
(3, 1)    0.4387767428592343
```
 
(2) 用 TfidfVectorizer
```
from sklearn.feature_extraction.text import TfidfVectorizer
 
tfidf2 = TfidfVectorizer()
corpus = ['This is the first document.',
          'This is the second second document.',
          'And the third one.',
          'Is this the first document?',
          ]
 
re = tfidf2.fit_transform(corpus)
print(re)
```

```
(0, 8)    0.4387767428592343
(0, 3)    0.4387767428592343
(0, 6)    0.35872873824808993
(0, 2)    0.5419765697264572
(0, 1)    0.4387767428592343
(1, 8)    0.27230146752334033
(1, 3)    0.27230146752334033
(1, 6)    0.2226242923251039
(1, 1)    0.27230146752334033
(1, 5)    0.8532257361452784
(2, 6)    0.2884767487500274
(2, 0)    0.5528053199908667
(2, 7)    0.5528053199908667
(2, 4)    0.5528053199908667
(3, 8)    0.4387767428592343
(3, 3)    0.4387767428592343
(3, 6)    0.35872873824808993
(3, 2)    0.5419765697264572
(3, 1)    0.4387767428592343
```
  
		可见，两种方法的结果是一样的。

#### 4.3 TF-IDF小结
	TF-IDF是常用的文本挖掘预处理基本步骤，但是如果预处理中使用了Hash Trick，则一般就无法使用TF-IDF了，因为Hash Trick后已经无法得到哈希后的各特征的IDF的值。使用了IF-IDF并标准化以后，就可以使用各个文本的词特征向量作为文本的特征，进行分类或者聚类分析。

	当然TF-IDF不光可以用于文本挖掘，在信息检索等很多领域都有使用。因此值得好好的理解这个方法的思想。