文本特征提取有两个非常重要的模型：

词集模型：单词构成的集合，集合自然每个元素都只有一个，也即词集中的每个单词都只有一个。

词袋模型：在词集的基础上如果一个单词在文档中出现不止一次，统计其出现的次数（频数）。

两者本质上的区别：词袋是在词集的基础上增加了频率的维度，词集只关注有和没有，词袋还要关注有几个。

假设我们要对一篇文章进行特征化，最常见的方式就是词袋。

## 1. 词集模型
       词集模型（Set of Words，简称SoW）： 单词构成的集合，每个单词只出现一次。和词袋模型唯一的不同是它仅仅考虑词是否在文本中出现，而不考虑词频。也就是一个词在文本在文本中出现1次和多次特征处理是一样的。在大多数时候，我们使用词袋模型，后面的讨论也是以词袋模型为主。

## 2. 词袋模型

       词袋模型(Bag of Words，简称BoW)，即将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的，把每一个单词都进行统计，同时计算每个单词出现的次数。也就是说，词袋模型不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重，而权重与词在文本中出现的频率有关。

	词袋模型的三部曲：分词（tokenizing），统计修订词特征值（counting）与标准化（normalizing）。

	词袋模型首先会进行分词，在分词之后，通过统计每个词在文本中出现的次数，我们就可以得到该文本基于词的特征，如果将各个文本样本的这些词与对应的词频放在一起，就是我们常说的向量化。向量化完毕后一般也会使用TF-IDF进行特征的权重修正，再将特征进行标准化。 再进行一些其他的特征工程后，就可以将数据带入机器学习算法进行分类聚类了。

	当然，词袋模型有很大的局限性，因为它仅仅考虑了词频，没有考虑上下文的关系，因此会丢失一部分文本的语义。但是大多数时候，如果我们的目的是分类聚类，则词袋模型表现的很好。

## 3. 词袋模型CountVectorizer实现

		直接用scikit-learn的CountVectorizer类来完成，这个类可以帮我们完成文本的词频统计与向量化。CountVectorize函数比较重要的几个参数为：

		1) decode_error，处理解码失败的方式，分为‘strict’、‘ignore’、‘replace’三种方式。
		2) strip_accents，在预处理步骤中移除重音的方式。默认为None，可设为ascii或unicode，将使用ascii或unicode编码在预处理步骤去除raw document中的重音符号。
		3) max_features，词袋特征个数的最大值。
		4) stop_words，设置停用词，设为english将使用内置的英语停用词，设为一个list可自定义停用词，设为None不使用停用词，设为None且max_df∈[0.7, 1.0)将自动根据当前的语料库建立停用词表。
		5) max_df，可以设置为范围在[0.0 1.0]的float，也可以设置为没有范围限制的int，默认为1.0。这个参数的作用是作为一个阈值，当构造语料库的关键词集的时候，如果某个词的document frequence大于max_df，这个词不会被当作关键词。如果这个参数是float，则表示词出现的次数与语料库文档数的百分比，如果是int，则表示词出现的次数。如果参数中已经给定了vocabulary，则这个参数无效。
		6) min_df，类似于max_df，参数min_df=n表示词必须要在至少n个文档中出现过，否则就不考虑。
		7) binary，默认为False，当与TF-IDF结合使用时需要设置为True。

       CountVectorizer是通过fit_transform()函数将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在第i个文本下的词频。即各个词语出现的次数，通过get_feature_names()可看到所有文本的关键字，通过toarray()可看到词频矩阵的结果。

#### 3.1 词袋化/向量化
```
from sklearn.feature_extraction.text import CountVectorizer
 
# 实例化分词对象
vec = CountVectorizer(min_df=1)
 
# 将文本进行词袋处理
corpus = ['This is the first document.',
        'This is the second second document.',
        'And the third one.',
        'Is this the first document?']
 
X = vec.fit_transform(corpus)
print(X)
```
       
       本例中处理的数据集均为英文，所以针对解码失败直接忽略，使用ignore方式，stop_words的方式使用english，strip_accents方式为ascii方式。
		对于上面4个文本的处理输出如下：

```
(0, 2)    1
(0, 3)    1
(0, 6)    1
(0, 4)    1
(0, 7)    1
(1, 5)    2
(1, 2)    1
(1, 6)    1
(1, 4)    1
(1, 7)    1
(2, 9)    1
(2, 0)    1
(2, 8)    2
(2, 1)    1
(3, 2)    1
(3, 3)    1
(3, 6)    1
(3, 4)    1
(3, 7)    1
```

		可以看出4个文本的词频已经统计出，输出中，左边的括号中的第一个数字是文本的序号，第2个数字是词的序号，注意词的序号是基于所有的文档的。第三个数字就是我们的词频。

		获取对应的特征名称，即各个特征代表的词：
```
fnames = vec.get_feature_names()
print(fnames)
```

```
['china', 'come', 'document', 'first', 'is', 'second', 'the', 'this', 'to', 'travel']
```

		获取词袋数据，即每个文本的词向量特征：

```
arr = X.toarray()
print(arr)
```

```
[[0 0 1 1 1 0 1 1 0 0]
 [0 0 1 0 1 2 1 1 0 0]
 [1 1 0 0 0 0 0 0 2 1]
 [0 0 1 1 1 0 1 1 0 0]]
```

		至此已经完成了词袋化。

		可以看到一共有10个词，所以4个文本都是10维的特征向量。而每一维的向量依次对应了上面的10个词。另外由于词"I"在英文中是停用词，不参加词频的统计。

	　 由于大部分的文本都只会使用词汇表中的很少一部分的词，因此词向量中会有大量的0。也就是说词向量是稀疏的。在实际应用中一般使用稀疏矩阵来存储。

	　  将文本做了词频统计后，我们一般会通过TF-IDF进行词特征值修订。

		向量化的方法很好用，也很直接，但是在有些场景下很难使用，比如分词后的词汇表非常大，达到100万+，此时如果我们直接使用向量化的方法，将对应的样本对应特征矩阵载入内存，有可能将内存撑爆，在这种情况下怎么办呢？第一反应是要进行特征的降维，说的没错！而Hash Trick就是非常常用的文本特征降维方法。

#### 3.2  使用现有词袋的特征，对其他文本进行特征提取      

		如何可以使用现有的词袋的特征，对其他文本进行特征提取呢？

       我们定义词袋的特征空间叫做词汇表vocabulary：vocabulary=vec.vocabulary_

       对其他文本进行词袋处理时，可以直接使用现有的词汇表：
```
vocabulary=vec.vocabulary_
new_vectorizer = CountVectorizer(min_df=1, vocabulary=vocabulary)
corpus2=["I come to China to travel",
    "This is a car polupar in China",
    "I love tea and Apple ",
    "The work is to write some papers in science"]
 
arr2 = new_vectorizer.fit_transform(corpus2).toarray()
print(arr2)
 
fnames2 = vec.get_feature_names()
print(fnames2)
```

```
[[1 1 0 0 0 0 0 0 2 1]
 [1 0 0 0 1 0 0 1 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 1 0 1 0 1 0]]
['china', 'come', 'document', 'first', 'is', 'second', 'the', 'this', 'to', 'travel']
```

		 可以看出，各个特征代表的词不变 ，还是之前词袋的特征。 
